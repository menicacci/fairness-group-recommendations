{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv4vjprw4AJ1rcvdc7fltn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menicacci/fairness-group-recommendations/blob/main/Group_Recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3muXF-Vp_EEM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import heapq\n",
        "import numpy as np\n",
        "import random\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets: https://grouplens.org/datasets/movielens/"
      ],
      "metadata": {
        "id": "2VlqAMm8L_2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset loading\n",
        "df_movies = pd.read_csv(r\"movies.csv\")\n",
        "df_ratings = pd.read_csv(r\"ratings.csv\")\n",
        "\n",
        "\n",
        "'''\n",
        "  Create a pandas df with a column for each value of the df[column_name] column (in this case reppresenting the movieId).\n",
        "  In this example, each row reppresents the ratings given by a user to the specific movie (NaN means rating unknown).\n",
        "'''\n",
        "df_user_based_cf = df_ratings.groupby('userId').apply(lambda x: x.set_index('movieId')['rating']).unstack(fill_value=np.nan)"
      ],
      "metadata": {
        "id": "tWRRBizK_O72"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Takes as input a pandas df and two row indexes.\n",
        "  For a given column c, if df[row_idx1] and df[row_idx2] are both populated, the pair will be included in the output array.\n",
        "  If the indexes are the same, it returns an empty array\n",
        "'''\n",
        "def find_non_null_column_pairs(df, row_idx1, row_idx2):\n",
        "  if row_idx1 == row_idx2:\n",
        "    return []\n",
        "\n",
        "  row1 = df.iloc[row_idx1].values\n",
        "  row2 = df.iloc[row_idx2].values\n",
        "\n",
        "  non_null_mask = ~np.isnan(row1) & ~np.isnan(row2)\n",
        "  return [(val1, val2) for val1, val2 in zip(row1[non_null_mask], row2[non_null_mask])]\n",
        "\n",
        "\n",
        "'''\n",
        "  Calculates the average value of a row (excluding NaN values)\n",
        "'''\n",
        "def average_value(df, row_idx):\n",
        "  return np.nanmean(df.iloc[row_idx].to_numpy())\n",
        "\n",
        "\n",
        "'''\n",
        "  Takes as input a pandas df and two row indexes.\n",
        "  Calculates the peason correlation between two items.\n",
        "'''\n",
        "def pearson_correlation(df, row_idx1, row_idx2):\n",
        "  common_items = find_non_null_column_pairs(df, row_idx1, row_idx2)\n",
        "  if not common_items:\n",
        "    return 0\n",
        "\n",
        "  mean_1 = average_value(df, row_idx1)\n",
        "  mean_2 = average_value(df, row_idx2)\n",
        "\n",
        "  n = sum((item[0] - mean_1) * (item[1] - mean_2) for item in common_items)\n",
        "  d1 = math.sqrt(sum((item[0] - mean_1)**2 for item in common_items))\n",
        "  d2 = math.sqrt(sum((item[1] - mean_2)**2 for item in common_items))\n",
        "\n",
        "  return n / (d1 * d2) if (d1 != 0 and d2 != 0) else 0\n",
        "\n",
        "\n",
        "'''\n",
        "  Takes as input a pandas df and two row indexes.\n",
        "  Calculates the cosine similarity between two items.\n",
        "'''\n",
        "def cosine_similarity(df, row_idx1, row_idx2):\n",
        "    common_items = find_non_null_column_pairs(df, row_idx1, row_idx2)\n",
        "    if not common_items:\n",
        "        return 0\n",
        "\n",
        "    dot_product = sum(item[0] * item[1] for item in common_items)\n",
        "    magnitude1 = math.sqrt(sum(item[0] ** 2 for item in common_items))\n",
        "    magnitude2 = math.sqrt(sum(item[1] ** 2 for item in common_items))\n",
        "\n",
        "    return dot_product / (magnitude1 * magnitude2) if (magnitude1 != 0 and magnitude2 != 0) else 0\n",
        "\n",
        "\n",
        "'''\n",
        "  Takes as input a pandas df, the index of the target item, the size of the neighbourhood and a score function.\n",
        "  Returns the neighbourhood that maximizes the score function ordered by the score itself.\n",
        "  Output type: [(a, b), ...] -> a: item score, b: item index.\n",
        "'''\n",
        "def get_max_similarity(df, target_idx, size, score_function):\n",
        "  top_scores_heap = []\n",
        "\n",
        "  for row_idx in range(len(df)):\n",
        "    score = score_function(df, target_idx, row_idx)\n",
        "    heapq.heappush(top_scores_heap, (score, row_idx))\n",
        "\n",
        "    if len(top_scores_heap) > size:\n",
        "      heapq.heappop(top_scores_heap)\n",
        "\n",
        "  return sorted(top_scores_heap, key=lambda x: x[0])"
      ],
      "metadata": {
        "id": "KeeQxAf7SR--"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Takes as input a pandas df, a column value, a list of neighbours and the mean of the target item values.\n",
        "  Returns a prediction score for the target item.\n",
        "  Neighbour's list structure: [(a, b, c), ...] -> a: score, b: index, c: mean\n",
        "'''\n",
        "def get_prediction_score(df, column, similar_items, target_mean):\n",
        "  similar_items_arr = np.array(similar_items, dtype=float)\n",
        "\n",
        "  item_indices = similar_items_arr[:, 1].astype(int)\n",
        "  item_similarities = similar_items_arr[:, 0]\n",
        "  item_means = similar_items_arr[:, 2]\n",
        "\n",
        "  df_values = df.iloc[item_indices][column].values\n",
        "  valid_indices = ~np.isnan(df_values)\n",
        "\n",
        "  n = np.sum(item_similarities[valid_indices] * (df_values[valid_indices] - item_means[valid_indices]))\n",
        "  d = np.sum(np.abs(item_similarities[valid_indices]))\n",
        "\n",
        "  return target_mean + (n / d) if d != 0 else None\n",
        "\n",
        "\n",
        "\n",
        "def refactor_similarities(df, similar_items):\n",
        "  return [(similar_item[0], similar_item[1], average_value(df, similar_item[1])) for similar_item in similar_items]\n",
        "\n",
        "\n",
        "'''\n",
        "  Takes as input a pandas df, a target item index, a list of neighbours and the desired output list length.\n",
        "  Returns a list of column values with the highest score.\n",
        "  Output type: [(a, b), ...] -> a: prediction score, b: item index.\n",
        "'''\n",
        "def get_predictions_based_on_similarity(df, target_item, similar_items, size, column_indexes=None):\n",
        "  prediction_scores = []\n",
        "\n",
        "  target_item_mean = average_value(df, target_item)\n",
        "  similar_items = refactor_similarities(df, similar_items)\n",
        "\n",
        "  row_item = df.iloc[target_item]\n",
        "\n",
        "  columns = df.columns if column_indexes is None else column_indexes\n",
        "  for column in columns:\n",
        "    if np.isnan(row_item[column]):\n",
        "      prediction = get_prediction_score(df, column, similar_items, target_item_mean)\n",
        "\n",
        "      if prediction is not None:\n",
        "        heapq.heappush(prediction_scores, (prediction, column))\n",
        "\n",
        "        if len(prediction_scores) > size:\n",
        "          heapq.heappop(prediction_scores)\n",
        "\n",
        "  return sorted(prediction_scores, key=lambda x: x[0])"
      ],
      "metadata": {
        "id": "R9YSiK0oI87E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_nan_columns_in_group(df, group):\n",
        "  unseen = {}\n",
        "\n",
        "  for item in group:\n",
        "    row_item = df.iloc[item]\n",
        "    unseen[item] = []\n",
        "\n",
        "    for column, index in df.items():\n",
        "      if np.isnan(row_item[column]):\n",
        "        unseen[item].append(column)\n",
        "\n",
        "  intersection_set = set(unseen[group[0]])\n",
        "  for key in group[1:]:\n",
        "    intersection_set.intersection_update(unseen[key])\n",
        "\n",
        "  return intersection_set\n",
        "\n",
        "\n",
        "def random_distinct_indexes(df, num):\n",
        "  distinct_indexes = df.index.tolist()\n",
        "  random.shuffle(distinct_indexes)\n",
        "\n",
        "  return distinct_indexes[:num]"
      ],
      "metadata": {
        "id": "S-89ny5eNOBY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_aggregation(prediction_scores):\n",
        "  return statistics.mean(prediction_scores)\n",
        "\n",
        "\n",
        "def least_misery(prediction_scores):\n",
        "  return min(prediction_scores)\n",
        "\n",
        "\n",
        "def get_combined_predictions(df, group, group_predictions, group_neighbours):\n",
        "  predictions_map = [{prediction[1]: prediction[0] for prediction in item_predictions} for item_predictions in group_predictions]\n",
        "\n",
        "  combined_predictions = {}\n",
        "  for item_predictions in predictions_map:\n",
        "    for prediction in item_predictions.keys():\n",
        "      combined_predictions[prediction] = []\n",
        "\n",
        "  average_values = [average_value(df, item) for item in group]\n",
        "  for prediction in combined_predictions.keys():\n",
        "    for item_index in range(len(group)):\n",
        "      if not prediction in predictions_map[item_index]:\n",
        "        new_prediction = get_prediction_score(df, prediction, group_neighbours[item_index], average_values[item_index])\n",
        "        if new_prediction is None:\n",
        "          new_prediction = 0\n",
        "\n",
        "        combined_predictions[prediction].append(new_prediction)\n",
        "      else:\n",
        "        combined_predictions[prediction].append(predictions_map[item_index][prediction])\n",
        "\n",
        "  return combined_predictions\n",
        "\n",
        "\n",
        "def get_group_recommendation(df, group, neighbourhood_size=50, item_prediction_size=100, predictions_size=10, group_score_function=average_aggregation, score_function=pearson_correlation):\n",
        "  group_neighbours = [refactor_similarities(df, get_max_similarity(df, target, neighbourhood_size, score_function)) for target in group]\n",
        "\n",
        "  nan_columns = find_nan_columns_in_group(df, group)\n",
        "  group_predictions = [get_predictions_based_on_similarity(df, target, similar_users, item_prediction_size, column_indexes=nan_columns)\n",
        "                       for target, similar_users in zip(group, group_neighbours)\n",
        "                      ]\n",
        "\n",
        "  combined_predictions = get_combined_predictions(df, group, group_predictions, group_neighbours)\n",
        "\n",
        "  predictions_scores = {prediction: [combined_predictions[prediction], group_score_function(combined_predictions[prediction])] for prediction in combined_predictions.keys()}\n",
        "  sorted_predictions = sorted(predictions_scores.items(), key=lambda x: x[1][1], reverse=True)\n",
        "\n",
        "  return sorted_predictions[:predictions_size]\n",
        "\n",
        "\n",
        "def get_group_recommendation_with_disagreements(df, group, predictions_size=10, group_score_function=average_aggregation):\n",
        "  predictions = get_group_recommendation(df, group, item_prediction_size=500, predictions_size=200, group_score_function=group_score_function)\n",
        "\n",
        "  group_size = len(group)\n",
        "  for prediction in predictions:\n",
        "    prediction[1][1] = ((1 / statistics.stdev(prediction[1][0]))**(1/group_size)) * prediction[1][1]\n",
        "\n",
        "  sorted_predictions = sorted(predictions, key=lambda x: x[1][1], reverse=True)\n",
        "  return sorted_predictions[:predictions_size]"
      ],
      "metadata": {
        "id": "FtMq1DPL6pX8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users =  random_distinct_indexes(df_user_based_cf, 3)\n",
        "\n",
        "standard_predictions = get_group_recommendation(df_user_based_cf, users, group_score_function=average_aggregation)\n",
        "predictions_with_disagrements = get_group_recommendation_with_disagreements(df_user_based_cf, users)"
      ],
      "metadata": {
        "id": "EWO-lS9teucf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_group_predictions(group_predictions):\n",
        "  for prediction in group_predictions:\n",
        "    print(f\"Movie ID:\\t{prediction[0]}\\t\\tScore: [{prediction[1][1]}]\\t Users' Predictions: {prediction[1][0]}\")\n",
        "\n",
        "print(\"Prediction without considering disagreements\")\n",
        "print_group_predictions(standard_predictions)\n",
        "\n",
        "print(\"\\nPrediction considering disagreements\")\n",
        "print_group_predictions(predictions_with_disagrements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKAzQQgVDXF2",
        "outputId": "c0b23339-eb2c-41b9-9183-5c5c3b5a6ba8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction without considering disagreements\n",
            "Movie ID:\t2300\t\tScore: [5.278852475436524]\t Users' Predictions: [4.716783216783217, 5.798452468680914, 5.321321740845441]\n",
            "Movie ID:\t3421\t\tScore: [5.100084459944183]\t Users' Predictions: [3.9761341739253937, 6.491140215716487, 4.832978990190668]\n",
            "Movie ID:\t1673\t\tScore: [5.094998476992824]\t Users' Predictions: [4.8701923076923075, 5.6939374185136895, 4.720865704772475]\n",
            "Movie ID:\t933\t\tScore: [4.973349437764876]\t Users' Predictions: [4.542051282051283, 5.174706649282921, 5.203290381960423]\n",
            "Movie ID:\t4011\t\tScore: [4.89961328939587]\t Users' Predictions: [4.340374820425755, 5.829128661709879, 4.529336386051978]\n",
            "Movie ID:\t1207\t\tScore: [4.796935747217571]\t Users' Predictions: [4.558070104844298, 5.353439680957129, 4.479297455851285]\n",
            "Movie ID:\t6711\t\tScore: [4.785359746750945]\t Users' Predictions: [4.273097131447958, 5.144940575608309, 4.938041533196569]\n",
            "Movie ID:\t1293\t\tScore: [4.767060584714325]\t Users' Predictions: [3.9851116625310175, 5.242263210368893, 5.073806881243064]\n",
            "Movie ID:\t82\t\tScore: [4.729184048519578]\t Users' Predictions: [4.320850202429149, 5.292331742636169, 4.574370200493416]\n",
            "Movie ID:\t1394\t\tScore: [4.684065058180334]\t Users' Predictions: [4.685668498168498, 4.491140215716487, 4.875386460656017]\n",
            "\n",
            "Prediction considering disagreements\n",
            "Movie ID:\t919\t\tScore: [12.168193007510643]\t Users' Predictions: [4.5825080325080325, 4.663488700564972, 4.560839096153866]\n",
            "Movie ID:\t7451\t\tScore: [11.42010511204988]\t Users' Predictions: [3.983806692191408, 3.9030720338983054, 3.9588336192109774]\n",
            "Movie ID:\t1073\t\tScore: [10.053394943291071]\t Users' Predictions: [4.3104072446727315, 4.274575729989583, 4.428110078254613]\n",
            "Movie ID:\t122904\t\tScore: [9.697954852851813]\t Users' Predictions: [4.0810102661788346, 3.943994730154277, 4.047652419458349]\n",
            "Movie ID:\t5989\t\tScore: [9.389026016439502]\t Users' Predictions: [3.938171332678309, 4.094831065096992, 4.020626493003851]\n",
            "Movie ID:\t916\t\tScore: [9.377987806914524]\t Users' Predictions: [4.042051282051282, 4.174706649282921, 4.203290381960423]\n",
            "Movie ID:\t99114\t\tScore: [8.815434973046052]\t Users' Predictions: [4.124502610667623, 3.9343220338983054, 4.052957322781686]\n",
            "Movie ID:\t112552\t\tScore: [8.640891406560907]\t Users' Predictions: [4.070627289377289, 4.2974172719935435, 4.206621545570332]\n",
            "Movie ID:\t115713\t\tScore: [8.564894384964397]\t Users' Predictions: [4.190132783882784, 4.443220835780365, 4.363672827621615]\n",
            "Movie ID:\t616\t\tScore: [8.174343772060045]\t Users' Predictions: [3.908828671328671, 4.088720265994258, 4.139565816537973]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "experiments = 10\n",
        "group_size = 3\n",
        "score_functions = [average_aggregation, least_misery]\n",
        "\n",
        "results_wo_dis = []\n",
        "results_wt_dis = []\n",
        "for _ in range(experiments):\n",
        "  users_exp = random_distinct_indexes(df_user_based_cf, group_size)\n",
        "  wo_dis = []\n",
        "  wt_dis = []\n",
        "\n",
        "  for function in score_functions:\n",
        "    wo_dis.append([pred[1][0] for pred in get_group_recommendation(df_user_based_cf, users_exp, item_prediction_size=500, group_score_function=function)])\n",
        "    wt_dis.append([pred[1][0] for pred in get_group_recommendation_with_disagreements(df_user_based_cf, users_exp, group_score_function=function)])\n",
        "\n",
        "  results_wo_dis.append(wo_dis)\n",
        "  results_wt_dis.append(wt_dis)"
      ],
      "metadata": {
        "id": "VOzrm2KJmXzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}